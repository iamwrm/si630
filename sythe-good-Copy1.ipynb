{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T10:09:32.247111Z",
     "start_time": "2020-04-29T10:09:31.000255Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jieba\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "import sklearn\n",
    "import scipy\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora import MalletCorpus\n",
    "from gensim.test.utils import get_tmpfile, common_texts\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from random import random, randint\n",
    "from glob import glob\n",
    "from math import log\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "kTOKENIZER = TreebankWordTokenizer()\n",
    "kDOC_NORMALIZER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:00:01.496675Z",
     "start_time": "2020-04-29T10:59:50.410687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./w2v/lit/en/1e.txt']\n",
      "1\n",
      "['./w2v/lit/cn/2c.txt']\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000000, 15105900)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_path = \"%s/*.txt\" % './w2v/lit/en'\n",
    "\n",
    "files = glob(search_path)\n",
    "print(files)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for i in files:\n",
    "    contents = open(i).read()\n",
    "    documents.append(contents)\n",
    "\n",
    "\n",
    "texts = [\n",
    "    [word for word in jieba.cut(document)] for document in documents\n",
    "]\n",
    "\n",
    "\n",
    "print(len(texts))\n",
    "\n",
    "model_en = Word2Vec(texts, size=100, window=5, min_count=1, workers=4)\n",
    "model_en.train(texts,total_examples=model_en.corpus_count, epochs=100)\n",
    "\n",
    "\n",
    "search_path = \"%s/*.txt\" % './w2v/lit/cn'\n",
    "\n",
    "files = glob(search_path)\n",
    "print(files)\n",
    "\n",
    "\n",
    "documents = []\n",
    "\n",
    "for i in files:\n",
    "    contents = open(i).read()\n",
    "    documents.append(contents)\n",
    "\n",
    "\n",
    "texts = [\n",
    "    [word for word in jieba.cut(document)] for document in documents\n",
    "]\n",
    "\n",
    "\n",
    "print(len(texts))\n",
    "\n",
    "model_cn = Word2Vec(texts, size=100, window=5, min_count=1, workers=4)\n",
    "model_cn.train(texts,total_examples=model_cn.corpus_count, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T10:16:27.091078Z",
     "start_time": "2020-04-29T10:16:27.080677Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_similar_vector(model_i,word):\n",
    "    try:\n",
    "        word = model_i.wv.most_similar(positive=[model_i.wv[word]], topn=2)[1][0]\n",
    "        most_similar_wv = model_i.wv[word]\n",
    "    except:\n",
    "        most_similar_wv = model_i.wv[random.choice(model_i.wv.index2entity)]\n",
    "    return most_similar_wv\n",
    "\n",
    "\n",
    "def get_word_for_m2_if_m1_i(model_1,model_2,word):\n",
    "    try:\n",
    "        word = model_2.wv.most_similar(positive=[model_1.wv[word]], topn=1)[0][0]\n",
    "        most_similar = word\n",
    "    except:\n",
    "#         wv = model_2.wv[random.choice(model_2.wv.index2entity)]\n",
    "        most_similar = random.choice(list(model_2.wv.vocab))\n",
    "#         print(\"r\",end=\"\")\n",
    "#         most_similar = model_2.most_similar(positive=wv, topn=1)[0][0]\n",
    "        \n",
    "    return most_similar\n",
    "\n",
    "\n",
    "# def get_word_for_m2_if_m1_i(model_1,model_2,word):\n",
    "#     most_similar = random.choice(list(model_2.wv.vocab))\n",
    "\n",
    "#     return most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T09:56:00.056123Z",
     "start_time": "2020-04-29T09:56:00.048152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'扔掉'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(model_cn.wv.vocab)\n",
    "random.choice(list(model_cn.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T09:56:00.062003Z",
     "start_time": "2020-04-29T09:56:00.057663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今冬'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# try:\n",
    "#     word = model.wv.most_similar(positive=[model.wv['pride']], topn=2)[1][0]\n",
    "#     most_similar_wv = model.wv[word]\n",
    "# except:\n",
    "#     most_similar_wv = model.wv[random.choice(model.wv.index2entity)]\n",
    "    \n",
    "# print(get_similar_vector(model_en,'pridedd'))\n",
    "\n",
    "\n",
    "get_word_for_m2_if_m1_i(model_cn,model_en,'年末')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T09:56:03.802898Z",
     "start_time": "2020-04-29T09:56:00.063551Z"
    }
   },
   "outputs": [],
   "source": [
    "# tw_pd_en = pd.read_csv(\"./base_line_classifier/tra.csv\",sep=',',encoding='latin-1',names=['class','ram','time','q','user_id','text'])\n",
    "\n",
    "# train_S = tw_pd_en['class']\n",
    "# train_ST = tw_pd_en['text']\n",
    "\n",
    "# def preprocessor(text):\n",
    "#     \"\"\" Return a cleaned version of text\n",
    "#     \"\"\"\n",
    "#     # Remove HTML markup\n",
    "#     text = re.sub('<[^>]*>', '', text)\n",
    "#     # Save emoticons for later appending\n",
    "#     emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "#     # Remove any non-word character and append the emoticons,\n",
    "#     # removing the nose character for standarization. Convert to lower case\n",
    "#     text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "#     return text\n",
    "\n",
    "\n",
    "# def tokenizer(text):\n",
    "#     words = text.split()\n",
    "# #     words = [get_word_for_m2_if_m1_i(model_cn,model_en,word) for word in words]\n",
    "#     return words\n",
    "\n",
    "# def tokenizer_porter(text):\n",
    "#     return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "\n",
    "\n",
    "# # tokenizer('A good day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T10:01:19.827771Z",
     "start_time": "2020-04-29T09:56:03.808845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:  4.0min finished\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 1.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__preprocessor': <function preprocessor at 0x7f0ef3a62290>, 'vect__tokenizer': <function tokenizer at 0x7f0ef3a62440>}\n",
      "Best accuracy: 0.802\n",
      "Accuracy in test: 0.803\n"
     ]
    }
   ],
   "source": [
    "# stop = stopwords.words('english')\n",
    "\n",
    "# # split the dataset in train and test\n",
    "# X = train_ST\n",
    "# y = train_S\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "# tfidf = TfidfVectorizer(strip_accents=None,\n",
    "#                         lowercase=False,\n",
    "#                         preprocessor=None)\n",
    "\n",
    "# lr_tfidf = Pipeline([('vect', tfidf),\n",
    "#                      ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "# param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "#                'vect__tokenizer': [tokenizer],\n",
    "#                'vect__preprocessor': [ preprocessor],\n",
    "#                'clf__penalty': ['l2'],\n",
    "#                'clf__C': [1.0]},\n",
    "#               ]\n",
    "\n",
    "# gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "#                            scoring='accuracy',\n",
    "#                            cv=5,\n",
    "#                            verbose=1,\n",
    "#                            n_jobs=3)\n",
    "\n",
    "# gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# print('Best parameter set: ' + str(gs_lr_tfidf.best_params_))\n",
    "# print('Best accuracy: %.3f' % gs_lr_tfidf.best_score_)\n",
    "\n",
    "# clf = gs_lr_tfidf.best_estimator_\n",
    "# print('Accuracy in test: %.3f' % clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T10:06:10.097786Z",
     "start_time": "2020-04-29T10:06:09.487288Z"
    }
   },
   "outputs": [],
   "source": [
    "tw_pd_cn = pd.read_csv(\"./base_line_classifier/cna.csv\",sep=',',encoding='utf-8',names=['weibo_id','time','id_name','text','ul','ul2','class'])\n",
    "\n",
    "train_S_cn = tw_pd_cn['class'].iloc[1:]\n",
    "train_ST_cn = tw_pd_cn['text'].iloc[1:]\n",
    "\n",
    "# train_ST_cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T10:06:04.476815Z",
     "start_time": "2020-04-29T10:06:04.470362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'twenty'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_for_m2_if_m1_i(model_cn,model_en,'年末')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T10:58:53.331877Z",
     "start_time": "2020-04-29T10:58:53.315461Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ST_cn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6f9415006e1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# x_begin = 14000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mx_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mX_cn_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ST_cn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_begin\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_begin\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_cn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0my_cn_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_S_cn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_begin\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_begin\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_ST_cn' is not defined"
     ]
    }
   ],
   "source": [
    "def tokenizer_cn(text):\n",
    "    words = list(jieba.cut(text))\n",
    "    words = [get_word_for_m2_if_m1_i(model_cn,model_en,word) for word in words]\n",
    "\n",
    "    text=  ' '.join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# x_begin = 14000\n",
    "x_begin = 100\n",
    "X_cn_test = train_ST_cn.iloc[x_begin+0:x_begin+500].apply(tokenizer_cn)\n",
    "y_cn_test = train_S_cn.iloc[x_begin+0:x_begin+500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T10:58:53.334252Z",
     "start_time": "2020-04-29T02:58:36.119Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = gs_lr_tfidf.predict(X_cn_test)\n",
    "# pred= [int(i) for i in pred/4]\n",
    "\n",
    "# print(pred)\n",
    "y_cn_test = list(y_cn_test)\n",
    "y_cn_test = [int(i) for i in y_cn_test]\n",
    "\n",
    "# print(y_cn_test)\n",
    "\n",
    "n,m=0,0\n",
    "\n",
    "for i,v in enumerate(pred):\n",
    "    if pred[i]==4 and y_cn_test[i]!=1:\n",
    "        m+=1\n",
    "    else:\n",
    "        n+=1\n",
    "print(n/(n+m))\n",
    "\n",
    "right,wrong=0,0\n",
    "\n",
    "for i,v in enumerate(y_cn_test):\n",
    "    if y_cn_test[i]==0:\n",
    "        continue\n",
    "    if y_cn_test[i]==1 and pred[i]==4:\n",
    "        right+=1\n",
    "    elif y_cn_test[i]==-1 and pred[i]==0:\n",
    "        right+=1\n",
    "    else:\n",
    "        wrong+=1\n",
    "        \n",
    "print(right/(right+wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T10:01:43.168721Z",
     "start_time": "2020-04-29T10:01:28.018800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8026979166666667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = gs_lr_tfidf.predict(X_test)\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "c = Counter(pred == y_test)\n",
    "c[True]/(c[True]+c[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T10:14:05.117861Z",
     "start_time": "2020-04-29T10:10:43.117215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  2.0min remaining:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.1min finished\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.606 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 1.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__preprocessor': <function preprocessor at 0x7f3a57f97050>, 'vect__tokenizer': <function tokenizer at 0x7f3a590ee560>}\n",
      "Best accuracy: 0.718\n",
      "Accuracy in test: 0.716\n"
     ]
    }
   ],
   "source": [
    "tw_pd = pd.read_csv(\"./base_line_classifier/cna.csv\",sep=',',encoding='utf-8',names=['weibo_id','time','id_name','text','ul','ul2','class'])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jieba\n",
    "\n",
    "train_S = tw_pd['class'].iloc[1:]\n",
    "train_ST = tw_pd['text'].iloc[1:]\n",
    "\n",
    "def preprocessor(text):\n",
    "    \"\"\" Return a cleaned version of text\n",
    "    \"\"\"\n",
    "    # Remove HTML markup\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    # Save emoticons for later appending\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    # Remove any non-word character and append the emoticons,\n",
    "    # removing the nose character for standarization. Convert to lower case\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = jieba.cut(text)\n",
    "    text=  ' '.join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = train_ST\n",
    "y = train_S\n",
    "\n",
    "X.fillna(method = 'ffill', inplace = True)\n",
    "y.fillna(method = 'ffill', inplace = True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__tokenizer': [tokenizer],\n",
    "               'vect__preprocessor': [ preprocessor],\n",
    "               'clf__penalty': ['l2'],\n",
    "               'clf__C': [1.0]}\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameter set: ' + str(gs_lr_tfidf.best_params_))\n",
    "print('Best accuracy: %.3f' % gs_lr_tfidf.best_score_)\n",
    "\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Accuracy in test: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T10:53:08.117883Z",
     "start_time": "2020-04-29T10:52:23.500239Z"
    }
   },
   "outputs": [],
   "source": [
    "tw_pd_en = pd.read_csv(\"./base_line_classifier/tra.csv\",sep=',',encoding='latin-1',names=['class','ram','time','q','user_id','text'])\n",
    "\n",
    "\n",
    "train_S_en = tw_pd_en['class'].iloc[1:]\n",
    "train_ST_en = tw_pd_en['text'].iloc[1:]\n",
    "\n",
    "# train_ST_cn\n",
    "\n",
    "def tokenizer_en(text):\n",
    "    words = text.split()\n",
    "    words = [get_word_for_m2_if_m1_i(model_en,model_cn,word) for word in words]\n",
    "\n",
    "    text=  ' '.join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:00:35.770117Z",
     "start_time": "2020-04-29T11:00:01.498791Z"
    }
   },
   "outputs": [],
   "source": [
    "# x_begin = 14000\n",
    "x_begin = 100\n",
    "length = 8000\n",
    "X_en_test = train_ST_en.iloc[x_begin+0:x_begin+length].apply(tokenizer_en)\n",
    "y_en_test = train_S_en.iloc[x_begin+0:x_begin+length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T11:00:38.675541Z",
     "start_time": "2020-04-29T11:00:35.772413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.3941908713692946\n"
     ]
    }
   ],
   "source": [
    "pred = gs_lr_tfidf.predict(X_en_test)\n",
    "# pred= [int(i) for i in pred/4]\n",
    "\n",
    "pred = [int(i) for i in pred]\n",
    "\n",
    "\n",
    "# print(pred)\n",
    "y_en_test = list(y_en_test)\n",
    "y_en_test = [int(i) for i in y_en_test]\n",
    "\n",
    "# print(y_en_test)\n",
    "\n",
    "n,m=0,0\n",
    "\n",
    "for i,v in enumerate(pred):\n",
    "    if pred[i]==4 and y_en_test[i]!=1:\n",
    "        m+=1\n",
    "    else:\n",
    "        n+=1\n",
    "print(n/(n+m))\n",
    "\n",
    "right,wrong=0,0\n",
    "\n",
    "for i,v in enumerate(y_en_test):\n",
    "    if pred[i]==0:\n",
    "        continue\n",
    "    if y_en_test[i]==4 and pred[i]==1:\n",
    "        right+=1\n",
    "    elif y_en_test[i]==0 and pred[i]==-1:\n",
    "        right+=1\n",
    "    else:\n",
    "        wrong+=1\n",
    "        \n",
    "print(right/(right+wrong))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
