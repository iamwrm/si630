{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T20:35:53.379451Z",
     "start_time": "2020-04-28T20:35:53.361439Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jieba\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "import sklearn\n",
    "import scipy\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora import MalletCorpus\n",
    "from gensim.test.utils import get_tmpfile, common_texts\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from random import random, randint\n",
    "from glob import glob\n",
    "from math import log\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "kTOKENIZER = TreebankWordTokenizer()\n",
    "kDOC_NORMALIZER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T20:49:53.866664Z",
     "start_time": "2020-04-28T20:49:46.373570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000000, 34807500)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_path = \"%s/*.txt\" % './w2v/lit/en'\n",
    "\n",
    "files = glob(search_path)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for i in files:\n",
    "    contents = open(i).read()\n",
    "    documents.append(contents)\n",
    "\n",
    "\n",
    "texts = [\n",
    "    [word for word in jieba.cut(document)] for document in documents\n",
    "]\n",
    "\n",
    "\n",
    "print(len(texts))\n",
    "\n",
    "model_en = Word2Vec(texts, size=100, window=5, min_count=1, workers=4)\n",
    "model_en.train(texts,total_examples=model.corpus_count, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T20:50:01.220940Z",
     "start_time": "2020-04-28T20:49:53.868740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000000, 15105900)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_path = \"%s/*.txt\" % './w2v/lit/cn'\n",
    "\n",
    "files = glob(search_path)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for i in files:\n",
    "    contents = open(i).read()\n",
    "    documents.append(contents)\n",
    "\n",
    "\n",
    "texts = [\n",
    "    [word for word in jieba.cut(document)] for document in documents\n",
    "]\n",
    "\n",
    "\n",
    "print(len(texts))\n",
    "\n",
    "model_cn = Word2Vec(texts, size=100, window=5, min_count=1, workers=4)\n",
    "model_cn.train(texts,total_examples=model.corpus_count, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T20:54:32.883888Z",
     "start_time": "2020-04-28T20:54:32.876826Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_similar_vector(model_i,word):\n",
    "    try:\n",
    "        word = model_i.wv.most_similar(positive=[model_i.wv[word]], topn=2)[1][0]\n",
    "        most_similar_wv = model_i.wv[word]\n",
    "    except:\n",
    "        most_similar_wv = model_i.wv[random.choice(model_i.wv.index2entity)]\n",
    "    return most_similar_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T20:54:48.199133Z",
     "start_time": "2020-04-28T20:54:48.192007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.6263696e-03  2.2747482e-03 -4.1053354e-04  3.6275804e-03\n",
      "  1.2554936e-03 -3.9693443e-03 -1.7822701e-04  1.3358082e-03\n",
      " -1.9464294e-03  2.7665666e-03 -1.1480190e-03  1.3480812e-03\n",
      "  3.7503797e-03 -1.8680529e-03 -1.9776255e-03 -2.7777026e-03\n",
      "  1.6252104e-03  4.7658924e-03  3.6692990e-03  4.4510602e-03\n",
      "  2.0423895e-03 -3.7964901e-03 -1.5479241e-03  9.3749084e-04\n",
      "  1.9532603e-03  1.8262930e-03 -4.6424796e-03  2.3688853e-03\n",
      "  1.5256767e-03 -1.8302690e-03 -2.7226042e-03 -3.5747819e-04\n",
      "  4.4828053e-03  2.0401783e-03 -5.5992516e-04 -1.8783150e-03\n",
      "  4.3989369e-03 -3.2833919e-03 -2.4042081e-03  1.1698157e-03\n",
      " -2.6866526e-03  4.2988225e-03  3.3111698e-03 -7.4235606e-04\n",
      "  3.5434638e-03 -4.7810171e-03 -1.4192410e-03 -2.1528089e-03\n",
      "  4.4101290e-03  2.0722512e-03  4.2216320e-04  8.4836257e-04\n",
      "  5.7819876e-04 -4.9428041e-03 -1.5675620e-04  1.6696197e-03\n",
      " -1.4490996e-03  1.9065524e-03 -2.6394129e-03 -3.3217783e-03\n",
      "  2.6886594e-03  9.1919173e-05  5.0500274e-04  3.3762597e-03\n",
      " -3.4516463e-03 -4.2687859e-03 -1.8606974e-03 -1.4037878e-03\n",
      " -2.9901061e-03 -3.7396983e-03 -2.1821621e-03  4.0371609e-03\n",
      " -5.4011116e-04  2.9559622e-03 -2.4140859e-03 -1.6324655e-03\n",
      " -4.1320450e-03 -4.7272365e-03  3.4454335e-03  1.8734004e-03\n",
      " -6.5902586e-04  2.3906203e-03 -4.7879526e-03 -4.4995961e-03\n",
      "  7.1768096e-04 -3.4423859e-03  2.1434571e-03 -3.9480574e-04\n",
      " -1.2821655e-04  4.1154488e-03  2.9879580e-03 -1.5415604e-03\n",
      "  2.8717523e-03  4.1756541e-03 -3.2589450e-03  4.2828955e-03\n",
      "  2.6694424e-03 -4.0975423e-04 -4.4400529e-03 -4.1345875e-03]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# try:\n",
    "#     word = model.wv.most_similar(positive=[model.wv['pride']], topn=2)[1][0]\n",
    "#     most_similar_wv = model.wv[word]\n",
    "# except:\n",
    "#     most_similar_wv = model.wv[random.choice(model.wv.index2entity)]\n",
    "    \n",
    "print(get_similar_vector(model_en,'pridedd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-28T13:02:05.894Z"
    }
   },
   "outputs": [],
   "source": [
    "tw_pd_en = pd.read_csv(\"./base_line_classifier/tra.csv\",sep=',',encoding='latin-1',names=['class','ram','time','q','user_id','text'])\n",
    "\n",
    "train_S = tw_pd_en['class']\n",
    "train_ST = tw_pd_en['text']\n",
    "\n",
    "def preprocessor(text):\n",
    "    \"\"\" Return a cleaned version of text\n",
    "    \"\"\"\n",
    "    # Remove HTML markup\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    # Save emoticons for later appending\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    # Remove any non-word character and append the emoticons,\n",
    "    # removing the nose character for standarization. Convert to lower case\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    words = text.split()\n",
    "    words = [tuple(get_similar_vector(model_en,word)) for word in words]\n",
    "    return words\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "\n",
    "\n",
    "# tokenizer('A good day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-28T13:01:48.892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "# split the dataset in train and test\n",
    "X = train_ST\n",
    "y = train_S\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__tokenizer': [tokenizer],\n",
    "               'vect__preprocessor': [ preprocessor],\n",
    "               'clf__penalty': ['l2'],\n",
    "               'clf__C': [1.0]},\n",
    "              ]\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameter set: ' + str(gs_lr_tfidf.best_params_))\n",
    "print('Best accuracy: %.3f' % gs_lr_tfidf.best_score_)\n",
    "\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Accuracy in test: %.3f' % clf.score(X_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
